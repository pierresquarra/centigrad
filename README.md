# Micrograd, just a little bigger

I added some more stuff to [micrograd](https://github.com/karpathy/micrograd).

## Features

> I use Module, Linear, etc. instead of Neuon, Layer, MLP

### Activations

- ReLU
- Sigmoid
- Softmax
- Tanh

### Losses

- MSE
- Categorical Cross Entropy
- Hinge

### Examples

- Both of the demos in the original repository
- MNIST, but it's really slow